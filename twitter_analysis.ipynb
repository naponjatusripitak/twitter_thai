{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import json\n",
    "import tweepy\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import fileinput\n",
    "import pickle\n",
    "from pandas import Series\n",
    "import datetime\n",
    "import pytz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store your twitter credentials\n",
    "\n",
    "twitter_cred = dict()\n",
    "\n",
    "# Enter your own consumer_key, consumer_secret, access_key and access_secret\n",
    "# Replacing the stars (\"********\")\n",
    "\n",
    "twitter_cred['CONSUMER_KEY'] = ''\n",
    "twitter_cred['CONSUMER_SECRET'] = ''\n",
    "twitter_cred['ACCESS_KEY'] = ''\n",
    "twitter_cred['ACCESS_SECRET'] = ''\n",
    "\n",
    "# Save the information to a json so that it can be reused in code without exposing\n",
    "# the secret info to public\n",
    "\n",
    "with open('twitter_credentials.json', 'w') as secret_info:\n",
    "    json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping protocol. Save this to scrape.py and run the script.\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_api():\n",
    "    with open('twitter_credentials.json') as cred_data:\n",
    "        info = json.load(cred_data)\n",
    "        consumer_key = info['CONSUMER_KEY']\n",
    "        consumer_secret = info['CONSUMER_SECRET']\n",
    "        access_key = info['ACCESS_KEY']\n",
    "        access_secret = info['ACCESS_SECRET']\n",
    "    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        sys.exit(-1)\n",
    "    return api\n",
    "    \n",
    "# Function for searching tweets    \n",
    "def tweet_search(api, query, max_tweets, max_id, since_id):\n",
    "    searched_tweets = []\n",
    "    try:\n",
    "        new_tweets = []\n",
    "        for tweet in tweepy.Cursor(api.search,q=query, count=100, since_id=str(since_id), max_id=str(max_id-1)).items(max_tweets):\n",
    "            new_tweets.append(tweet)\n",
    "        print('found',len(new_tweets),'tweets', api.rate_limit_status()['resources']['search'])\n",
    "        if not new_tweets:\n",
    "            print('no tweets found')\n",
    "            return searched_tweets, max_id\n",
    "        else:\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "    except tweepy.TweepError:\n",
    "        print('exception raised, waiting 15 minutes')\n",
    "        print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "        time.sleep(15*60)      \n",
    "    return searched_tweets, max_id\n",
    "\n",
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "    ''' Function that gets the ID of a tweet. This ID can then be\n",
    "        used as a 'starting point' from which to search. The query is\n",
    "        required and has been set to a commonly used word by default.\n",
    "        The variable 'days_ago' has been initialized to the maximum\n",
    "        amount we are able to search back in time (9).'''\n",
    "\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id\n",
    "        \n",
    "def write_tweets(tweets, filename):\n",
    "    ''' Function that appends tweets to a file. '''\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "def main():\n",
    "    ''' This is a script that continuously searches for tweets\n",
    "        that were created over a given number of days. The search\n",
    "        dates and search phrase can be changed below. '''\n",
    "\n",
    "\n",
    "\n",
    "    ''' search variables: '''\n",
    "    search_phrases = ['#ไทยรักษาชาติ', '#ทรงพระสเลนเดอร์', '#เลือกตั้ง62', '#เลือกตั้งปี62', '#แม่มาแล้วธานอส', '#พระราชโองการ', '#ทรงพระสแลนด์เดอร์']\n",
    "    time_limit = 240                          # runtime limit in hours\n",
    "    max_tweets = 10000                           # number of tweets per search (will be\n",
    "                                               # iterated over) - maximum is 100\n",
    "    min_days_old, max_days_old = 2, 4          # search limits e.g., from 7 to 8\n",
    "                                               # gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    #USA = '39.8,-95.583068847656,2500km'       # this geocode includes nearly all American\n",
    "                                               # states (and a large portion of Canada)\n",
    "    \n",
    "\n",
    "    # loop over search items,\n",
    "    # creating a new file for each\n",
    "    for search_phrase in search_phrases:\n",
    "\n",
    "        print('Search phrase =', search_phrase)\n",
    "\n",
    "        ''' other variables '''\n",
    "        name = search_phrase.split()[0]\n",
    "        json_file_root = name + '/'  + name\n",
    "        os.makedirs(os.path.dirname(json_file_root), exist_ok=True)\n",
    "        read_IDs = False\n",
    "        \n",
    "        # open a file in which to store the tweets\n",
    "        if max_days_old - min_days_old == 1:\n",
    "            d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        else:\n",
    "            d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "            d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "                  d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "        json_file = json_file_root + '_' + day + '.json'\n",
    "        if os.path.isfile(json_file):\n",
    "            print('Appending tweets to file named: ',json_file)\n",
    "            read_IDs = True\n",
    "        \n",
    "        # authorize and load the twitter API\n",
    "        api = load_api()\n",
    "        \n",
    "        # set the 'starting point' ID for tweet collection\n",
    "        if read_IDs:\n",
    "            # open the json file and get the latest tweet ID\n",
    "            with open(json_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                max_id = json.loads(lines[-1])['id']\n",
    "                print('Searching from the bottom ID in file')\n",
    "        else:\n",
    "            # get the ID of a tweet that is min_days_old\n",
    "            if min_days_old == 0:\n",
    "                max_id = -1\n",
    "            else:\n",
    "                max_id = get_tweet_id(api, days_ago=(min_days_old-1))\n",
    "        # set the smallest ID to search for\n",
    "        since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "        print('max id (starting point) =', max_id)\n",
    "        print('since id (ending point) =', since_id)\n",
    "        \n",
    "\n",
    "\n",
    "        ''' tweet gathering loop  '''\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=time_limit)\n",
    "        count, exitcount = 0, 0\n",
    "        while dt.datetime.now() < end:\n",
    "            count += 1\n",
    "            print('count =',count)\n",
    "            # collect tweets and update max_id\n",
    "            tweets, max_id = tweet_search(api, search_phrase, max_tweets,\n",
    "                                          max_id=max_id, since_id=since_id)\n",
    "            # write tweets to file in JSON format\n",
    "            if tweets:\n",
    "                write_tweets(tweets, json_file)\n",
    "                exitcount = 0\n",
    "            else:\n",
    "                exitcount += 1\n",
    "                if exitcount == 3:\n",
    "                    if search_phrase == search_phrases[-1]:\n",
    "                        sys.exit('Maximum number of empty tweet strings reached - exiting')\n",
    "                    else:\n",
    "                        print('Maximum number of empty tweet strings reached - breaking')\n",
    "                        break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting hashtags into lists\n",
    "def hash_parse(tweet):\n",
    "    hashes = list()\n",
    "    for hashtag in tweet:\n",
    "        text = hashtag['text']\n",
    "        hashes.append(text)\n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON files, go through each line, extract data, and write to csv \n",
    "tweet_files = ['#ทรงพระสเลนเดอร์/#ทรงพระสเลนเดอร์_2019-02-07_to_2019-02-08.json',\n",
    "               '#ทรงพระแคนเซิล/#ทรงพระแคนเซิล_2019-02-07_to_2019-02-08.json',\n",
    "               '#ไทยรักษาชาติ/#ไทยรักษาชาติ_2019-02-07_to_2019-02-08.json',\n",
    "               '#พระราชโองการ/#พระราชโองการ_2019-02-07_to_2019-02-08.json',\n",
    "               '#เลือกตั้ง62/#เลือกตั้ง62_2019-02-07_to_2019-02-08.json',\n",
    "               '#เลือกตั้งปี62/#เลือกตั้งปี62_2019-02-07_to_2019-02-08.json',\n",
    "               '#แม่มาแล้วธานอส/#แม่มาแล้วธานอส_2019-02-07_to_2019-02-08.json'\n",
    "              ]\n",
    "\n",
    "f = open('tweets.csv', 'w')\n",
    "writer = csv.writer(f)\n",
    "for file in tweet_files:\n",
    "    print(file)\n",
    "    with open(file, 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            tweets = []\n",
    "            data = json.loads(line)\n",
    "            tweets.extend([data['id'], data['created_at'], data['user']['screen_name'], data['user']['id'],\n",
    "                           data['text'], data['retweet_count'], data['favorite_count'], data['in_reply_to_status_id'],\n",
    "                           data['in_reply_to_user_id'], data['user']['location'], data['place']['full_name']\n",
    "                           if data['place'] != None else '',\n",
    "                           hash_parse(data['entities']['hashtags']), data['place']['country_code']\n",
    "                           if data['place'] != None else '', data['coordinates']['coordinates'][0]\n",
    "                           if data['coordinates'] != None else 'NaN',\n",
    "                           data['coordinates']['coordinates'][1]\n",
    "                            if data['coordinates'] != None else 'NaN'\n",
    "                          ])\n",
    "            writer.writerow(tweets)            \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function\n",
    "keys =  ['id', 'created_at', 'user_name', 'user_id', 'text', 'retweet_count', 'favorite_count', 'in_reply_to_status_id',\n",
    "              'in_reply_to_user_id', 'user_location', 'place', 'hashtags', 'country_code', 'long', 'latt']\n",
    "df= pd.read_csv('tweets.csv', names = keys, converters={'hashtags': eval}) # use eval in order to retain list type object for hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates. This is an important step since a single tweet may contain multiple hashtags and we use the search api with hashtags as our query.\n",
    "df = df.drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting hashtags\n",
    "def count_hashtags(df):\n",
    "    hashtag_list = []                          #CREATE EMPTY LIST \n",
    "    for i in df.hashtags:    #LOOP OVER EVERY CELL IN ENTITIES_HASHTAGS\n",
    "        j = ', '.join(i)\n",
    "        if pd.notnull(j):                      #IF CELL NOT EMPTY\n",
    "            tags = j.split()                   #SPLIT EACH CELL INTO SEPARATE HASHTAGS\n",
    "            for t in tags:                     #FOR EACH TAG IN THE CELL\n",
    "                t = \"#\"+t                      #ADD '#' SYMBOL TO BEGINNING OF EACH TAG\n",
    "                t = t.replace(',', '')         #REMOVE COMMAS FROM END OF TAGS\n",
    "                t = t.lower()                  #MAKE TAG LOWER CASE\n",
    "                hashtag_list.append(t)         #ADD TAG TO OUR LIST\n",
    "    top = pd.DataFrame(Series(hashtag_list).value_counts(), columns=['count'])\n",
    "    return(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002446, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#ไทยรักษาชาติ', '#ทรงพระสเลนเดอร์', '#เลือกตั้ง62', '#เลือกตั้งปี62', '#อนาคตใหม่', '#แม่มาแล้วธานอส', '#พระราชโองการ', '#ทรงพระสแลนด์เดอร์', '#พรรคอนาคตใหม่', '#เลือกตั้ง2562']\n"
     ]
    }
   ],
   "source": [
    "# Count hashtags and store a list of top hashtags in tags\n",
    "df_top = count_hashtags(df)\n",
    "tags = df_top[0:10].index.tolist()\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating columns of hashtags\n",
    "for tag in tags:\n",
    "    df[tag] = df.hashtags.apply(lambda x: 1 if tag[1:] in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting hashtags per minute\n",
    "df['time'] = pd.DatetimeIndex(df['created_at'])\n",
    "grouper = df.groupby([pd.Grouper(key='time', freq = '1Min')])\n",
    "df1 = grouper[[s for s in tags]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from wide to long format & adjust for time difference\n",
    "df1.reset_index(level=0, inplace=True)\n",
    "df1 = df1.melt(id_vars='time')\n",
    "df1['time'] = pd.to_datetime(df1['time'], format='%Y-%b-%d %H:%M:%S.%f').dt.tz_localize('UTC').dt.tz_convert('Asia/Bangkok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove timezone just in case\n",
    "df1['time'] = df1['time'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in with plotly credentials\n",
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "key= ''\n",
    "plotly.tools.set_credentials_file(username='taozaze', api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data for plotly\n",
    "d={}\n",
    "data = []\n",
    "for tag in tags:\n",
    "        d[\"trace_{0}\".format(tag)]= go.Scatter(\n",
    "            x = df1[df1.variable == tag]['time'],\n",
    "            y = df1[df1.variable == tag]['value'],\n",
    "            mode = 'lines',\n",
    "            name = tag\n",
    "        )\n",
    "        data.append(d[\"trace_{0}\".format(tag)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~taozaze/7.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layout and plot\n",
    "layout = dict(title = 'Tweet Time Distribution by Hashtag',\n",
    "              xaxis = dict(title = 'Time'),\n",
    "              yaxis = dict(title = 'Tweets Per Minute'),\n",
    "              )\n",
    "\n",
    "py.iplot(go.Figure(data=data, layout=layout), filename = 'basic-line', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~taozaze/9.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat bar plot for top hashtags\n",
    "\n",
    "top_hash = df_top.reset_index()[0:10]\n",
    "data = [\n",
    "    go.Bar(\n",
    "        x=top_hash['index'], # assign x as the dataframe column 'x'\n",
    "        y=top_hash['count']\n",
    "    )\n",
    "]\n",
    "\n",
    "# Layout and plot\n",
    "layout = dict(title = 'Top Hashtags',\n",
    "              xaxis = dict(title = 'Hashtag'),\n",
    "              yaxis = dict(title = 'Hashtag Frequency'),\n",
    "              )\n",
    "\n",
    "py.iplot(go.Figure(data=data, layout=layout), filename = 'pandas-bar-chart', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
